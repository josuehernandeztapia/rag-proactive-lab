#!/usr/bin/env python3
"""Construct HASE training dataset merging consumption features with default labels."""

from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Iterable

import pandas as pd

DEFAULT_FEATURE_COLS = [
    "plaza_limpia",
    "placa",
    "fecha_dia",
    "litros_diarios",
    "litros_7d",
    "litros_14d",
    "litros_30d",
    "tickets_diarios",
    "tickets_7d",
    "tickets_14d",
    "tickets_30d",
    "recaudo_diario",
    "recaudo_7d",
    "recaudo_14d",
    "recaudo_30d",
    "precio_promedio",
    "recencia_dias",
    "credito_diario",
    "credito_7d",
    "credito_14d",
    "credito_30d",
    "coverage_ratio_7d",
    "coverage_ratio_14d",
    "coverage_ratio_30d",
    "downtime_days_14d",
    "downtime_days_30d",
    "protections_applied_last_12m",
    "days_since_last_protection",
]


def parse_args(argv: Iterable[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Build HASE training dataset")
    parser.add_argument(
        "--features",
        type=Path,
        default=Path("data/hase/consumos_features_daily.csv.gz"),
        help="Daily feature table generated by build_consumo_features.py",
    )
    parser.add_argument(
        "--labels",
        type=Path,
        required=True,
        help="CSV with at least columns {placa, default_flag} and optional label_date",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("data/hase/hase_training_dataset.csv"),
        help="Path to write merged training dataset",
    )
    parser.add_argument(
        "--observation-window",
        type=int,
        default=30,
        help="Number of days before label_date to enforce observation window",
    )
    return parser.parse_args(argv)


def load_features(path: Path) -> pd.DataFrame:
    if not path.exists():
        raise FileNotFoundError(f"Features file not found: {path}")
    df = pd.read_csv(path)
    if "last_protection_at" in df.columns and "days_since_last_protection" not in df.columns:
        df["last_protection_at"] = pd.to_datetime(
            df["last_protection_at"], errors="coerce", utc=True
        ).dt.tz_localize(None)
        reference_date = pd.Timestamp.utcnow().normalize()
        df["days_since_last_protection"] = df["last_protection_at"].apply(
            lambda value: (reference_date - value).days if pd.notnull(value) else -1
        )
        df = df.drop(columns=["last_protection_at"])
    if "protections_applied_last_12m" not in df.columns:
        df["protections_applied_last_12m"] = 0
    missing = [col for col in DEFAULT_FEATURE_COLS if col not in df.columns]
    if missing:
        raise ValueError(f"Missing required feature columns: {missing}")
    df["fecha_dia"] = pd.to_datetime(df["fecha_dia"], errors="coerce")
    df = df.dropna(subset=["fecha_dia", "placa"])
    return df


def load_labels(path: Path) -> pd.DataFrame:
    df = pd.read_csv(path)
    for col in ["placa", "default_flag"]:
        if col not in df.columns:
            raise ValueError(f"Label file must contain column '{col}'")
    if "label_date" not in df.columns:
        df["label_date"] = pd.Timestamp.today().normalize()
    else:
        df["label_date"] = pd.to_datetime(df["label_date"], errors="coerce")
        df = df.dropna(subset=["label_date"])
    df["default_flag"] = df["default_flag"].astype(int)
    return df


def merge_datasets(features: pd.DataFrame, labels: pd.DataFrame, window_days: int) -> pd.DataFrame:
    # Sort features for efficient merge
    features_sorted = features.sort_values(["placa", "fecha_dia"])

    merged_rows = []
    for _, label_row in labels.iterrows():
        placa = label_row["placa"]
        cutoff = label_row["label_date"]
        subset = features_sorted[features_sorted["placa"] == placa]
        if cutoff is not pd.NaT:
            subset = subset[subset["fecha_dia"] <= cutoff]
        if window_days:
            subset = subset[subset["fecha_dia"] >= cutoff - pd.Timedelta(days=window_days)]
        if subset.empty:
            continue
        latest = subset.sort_values("fecha_dia").iloc[-1]
        merged = latest.to_dict()
        merged.update({
            "label_date": cutoff,
            "default_flag": label_row["default_flag"],
        })
        for extra_col in label_row.index:
            if extra_col not in {"placa", "default_flag", "label_date"}:
                merged[f"label_{extra_col}"] = label_row[extra_col]
        merged_rows.append(merged)

    if not merged_rows:
        raise ValueError("No matching rows found between features and labels")

    merged_df = pd.DataFrame(merged_rows)
    merged_df = merged_df.sort_values(["placa", "label_date"])
    return merged_df


def _safe_write(df: pd.DataFrame, path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    suffix = path.suffix.lower()
    if suffix == ".parquet":
        try:
            df.to_parquet(path, index=False)
            return
        except ImportError:
            path = path.with_suffix(".csv")
    if suffix == ".gz":
        df.to_csv(path, index=False, compression="gzip")
    else:
        df.to_csv(path, index=False)


def main(argv: Iterable[str] | None = None) -> int:
    args = parse_args(argv)
    features = load_features(args.features)
    labels = load_labels(args.labels)
    merged = merge_datasets(features, labels, args.observation_window)
    _safe_write(merged, args.output)
    print(f"Training dataset saved to {args.output}")
    print(f"Rows: {len(merged)} | Columns: {len(merged.columns)}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
